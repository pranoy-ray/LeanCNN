# -*- coding: utf-8 -*-
"""TotalECNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/133diQQKAMk4PEnkoBIHmINTubsRkW4wx
"""

import os
import h5py
import math
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import matplotlib.pyplot as plt
from torch.utils.data import Dataset, DataLoader, Subset
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from torch.nn import Linear, PReLU, MSELoss,LeakyReLU, Sequential, Conv3d, Module, BatchNorm2d, Dropout, ReLU, PoissonNLLLoss, L1Loss, HuberLoss

class H5Dataset(Dataset):
    def __init__(self, file_path, dataset_name, target_name, label_name):
        self.file_path = file_path
        self.dataset_name = dataset_name
        #self.dataset_name2 = dataset_name2
        self.target_name = target_name
        self.label_name = label_name

        with h5py.File(self.file_path, 'r') as f:
            #t = torch.from_numpy(f[self.dataset_name][:])
            #m = t.mean(axis=0, keepdim=True)
            #s = t.std(axis=0, unbiased=False, keepdim=True)
            #t -= m
            #t /= s
            self.inputs = f[self.dataset_name][:]
            #self.inputs2 = f[self.dataset_name2][:]
            t = torch.from_numpy(f[self.target_name][:])
            #self.m = t.mean(axis=0, keepdim=True)
            #self.s = t.std(axis=0, unbiased=False, keepdim=True)
            #t -= self.m
            #t /= self.s
            self.targets = t
            self.labels = f[self.label_name][:]

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        return torch.FloatTensor(self.inputs[idx]), torch.FloatTensor([self.targets[idx]]), torch.FloatTensor([self.labels[idx]])#, self.m, self.s#torch.FloatTensor([self.m[idx]]), torch.FloatTensor([self.s[idx]])#torch.FloatTensor(self.inputs2[idx]),

def create_data_loaders(file_path, dataset_name, dataset_name2, target_name, label_name, batch_size=5):
    # Create the full dataset
    full_dataset = H5Dataset(file_path, dataset_name, target_name, label_name)
    #full_dataset = H5Datasetn(file_path, dataset_name, dataset_name2, target_name, label_name)

    # Get the total number of samples
    num_samples = len(full_dataset)

    # Create indices for train, validation, and test sets
    indices = list(range(num_samples))
    train_split = int(0.8*num_samples)
    train_val_split = int(0.9 * num_samples)

    # Shuffle the indices
    np.random.shuffle(indices)

    # Split the indices
    train_indices = indices[:train_split]
    val_indices = indices[train_split:train_val_split]
    test_indices = indices[train_val_split:]

    # Create Subset datasets
    train_dataset = Subset(full_dataset, train_indices)
    val_dataset = Subset(full_dataset, val_indices)
    test_dataset = Subset(full_dataset, test_indices)

    # Create DataLoaders
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size)
    test_loader = DataLoader(test_dataset, batch_size=batch_size)

    return train_loader, val_loader, test_loader, full_dataset

#Normal CNN
class chgnet3D(nn.Module):
    def __init__(self):
        super(chgnet3D, self).__init__()


        #Good CNN
        self.conv1 = Conv3d(1, 64, 5, stride=2)#,stride=2)#weight_norm(Conv3d(2, 12, 5,stride=2)) #23
        #self.batchnorm=nn.BatchNorm3d(64)
        self.act1 = nn.PReLU(64)#nn.ReLU()#nn.PReLU(64)#nn.GELU()#nn.PReLU(12)
        #self.act1 = nn.LeakyReLU()
        self.conv2 = Conv3d(64, 32, 3)#,stride=2)#weight_norm(Conv3d(12, 80, 3)) #21
        self.act2 = nn.PReLU(32)#nn.ReLU()#nn.PReLU(32)#nn.GELU()#nn.PReLU(80)
        #self.act2 = nn.LeakyReLU()
        self.conv3 = Conv3d(32, 16, 3)#,stride=2)#weight_norm(Conv3d(80, 160, 3)) #19
        self.act3 = nn.PReLU(16)#nn.ReLU()#nn.PReLU(16)#nn.GELU()#nn.PReLU(160)
        #self.act3 = nn.LeakyReLU()
        self.conv4 = Conv3d(16, 8, 3)#,stride=2)#weight_norm(Conv3d(160, 128, 3)) #17
        self.act4 = nn.PReLU(8)#nn.ReLU()#nn.PReLU(8)#nn.GELU()#nn.PReLU(128)
        #self.act4 = nn.LeakyReLU()
        self.conv5 = Conv3d(8, 1, 1)#,stride=2)#weight_norm(Conv3d(128, 128, 1)) #15
        #self.act5 = nn.PReLU(4)
        #self.conv6 = Conv3d(4, 2, 3) #11
        #self.act6 = nn.PReLU(2)
        #self.conv7 = Conv3d(2, 1, 1) #11
        self.mean = Mean()

    def forward(self, x):#, L):
        #x=x.squeeze()
        #first convolution
        x=self.conv1(x)#)weight_norm
        #x=self.batchnorm(x)
        x=self.act1(x)
        #x = self.dropout(x)
        #print(x.shape)
        #second convolution
        x=self.conv2(x)
        x=self.act2(x)
        #x = self.dropout(x)
        #print(x.shape)
        #third convolution
        x=self.conv3(x)
        x=self.act3(x)
        #x = self.dropout(x)
        #print(x.shape)
        #fourth convolution
        x=self.conv4(x)
        x=self.act4(x)
        #x = self.dropout(x)
        #print(x.shape)
        #fifth convolution
        x=self.conv5(x)
        #x=self.act5(x)
        #sixth convolution
        #x=self.conv6(x)
        #x=self.act6(x)
        #seventh convolution
        #x=self.conv7(x)
        #x=self.act7(x)
        x=self.mean(x)
        return x#.reshape(-1,1)#.flatten()#No reshape if FC layer active

# create a nn class (just-for-fun choice :-)
class RMSELoss(nn.Module):
    def __init__(self):
        super().__init__()
        self.mse = nn.MSELoss()

    def forward(self,yhat,y):
        return torch.abs(torch.sqrt(self.mse(yhat,y)))

class MAPELoss(nn.Module):
    def __init__(self):
        super().__init__()
        self.mae = nn.L1Loss()

    def forward(self,yhat,y):
        epsilon = 1e-8  # To avoid division by zero
        return torch.mean(torch.abs((yhat - y) / (y + epsilon))) * 100.0

class Mean(nn.Module):
    def __init__(self):
        super(Mean,self).__init__()

    def forward(self,x):
        return torch.mean(x,(2,3,4))

class CNN3D(nn.Module):
    def __init__(self, input_channels):
        super(CNN3D, self).__init__()
        self.conv1 = nn.Conv3d(input_channels, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv3d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool3d(2)
        self.fc1 = nn.Linear(64 * 3 * 3 * 3, 128)
        self.fc2 = nn.Linear(128, 1)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        x = F.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(-1, 64 * 3 * 3 * 3)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

dir = "results"
#os.chdir(dir)
mode1 = "2ptstats" #2ptstats #ECD
mode2 = "ECD" #2ptstats #ECD
target = "totale"
unit = "eV/atom"
batchsize = 50

# Create datasets and dataloaders
train_loader, val_loader, test_loader, full_dataset = create_data_loaders('/content/drive/MyDrive/alldata.h5', dataset_name=mode1, dataset_name2=mode2, target_name=target, label_name="JID", batch_size=batchsize)

# Print some information about the datasets
print(f"Total dataset size: {len(full_dataset)}")
print(f"Train set size: {len(train_loader.dataset)}")
print(f"Validation set size: {len(val_loader.dataset)}")
print(f"Test set size: {len(test_loader.dataset)}")
print(f"Input shape: {full_dataset.inputs.shape[1:]}")

num_epochs = 650#340#1000
lr = 0.005

# Assuming input shape is (batch_size, channels, depth, height, width)
# Create the model
#input_channels = full_dataset.inputs.shape[1]
#input_size = full_dataset.inputs.shape[2:]  # (depth, height, width)
#model = CNN3D(input_channels, input_size)
model = chgnet3D()

# Print model summary
num_params = count_parameters(model)
print(f"Model architecture: {model}")
print(f"Model has {num_params} parameters!")

criterion = MSELoss()#MAPELoss() #RMSELoss() #MAPELoss() #RMSELoss() #MSELoss()
optimizer = optim.Adam(model.parameters(),lr=lr)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

train_losses = []
val_losses = []

for epoch in range(num_epochs):
    model.train()
    train_loss = 0
    #for inputs, inputs2, targets, jid_batch in train_loader:
    for inputs, targets, jid_batch in train_loader:
        inputs = torch.unsqueeze(inputs,1)
        inputs, targets = inputs.to(device), targets.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    train_loss /= len(train_loader)
    train_losses.append(train_loss)
    if scheduler!=None:
        scheduler.step()

    model.eval()
    val_loss = 0
    with torch.no_grad():
        #for inputs, inputs2, targets, jid_batch in val_loader:
        for inputs, targets, jid_batch in val_loader:
            inputs = torch.unsqueeze(inputs,1)
            inputs, targets = inputs.to(device), targets.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            val_loss += loss.item()
    val_loss /= len(val_loader)
    val_losses.append(val_loss)

    print(f"Epoch {epoch+1}/{num_epochs}, LR {optimizer.param_groups[0]['lr']:.4f}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

def evaluate_model(model, dataloader):
    model.eval()
    predictions = []
    targets = []
    jid_test = []
    with torch.no_grad():
        #for inputs, inputs2, batch_targets, jid_batch in dataloader:
        for inputs, batch_targets, jid_batch in dataloader:
            inputs = torch.unsqueeze(inputs,1)
            inputs = inputs.to(device)
            outputs = model(inputs)
            predictions.extend(outputs.cpu().numpy())
            targets.extend(batch_targets.numpy())
            jid_test.extend(torch.flatten(jid_batch).tolist())
    return np.array(predictions), np.array(targets), np.array(jid_test)

train_predictions, train_targets, jid_train = evaluate_model(model, train_loader)
test_predictions, test_targets, jid_test = evaluate_model(model, test_loader)

#Training

# Mean Absolute Error (MAE)
print("Training Set:")
mae = mean_absolute_error(train_targets, train_predictions)
mae = round(mae,2)
print("MAE:", mae, unit)

# Mean Squared Error (MSE)
mse = mean_squared_error(train_targets, train_predictions)
mse = round(mse,2)
print("MSE:", mse)
# Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)
rmse = round(rmse,2)
print("RMSE:", rmse, unit)

# R-squared (R2)
r2 = r2_score(train_targets, train_predictions)
r2 = round(r2,3)
print("R2:", r2)

# Mean Absolute Percentage Error (MAPE)
# Scikit-learn doesn't have a built-in MAPE function
# We can calculate manually:
mape = np.mean(np.abs((np.array(train_targets) - np.array(train_predictions)) / np.average(train_targets))) * 100.0
mape = round(mape,2)
print("MAPE:", mape, "%")


#Testing
print("Testing Set:")
# Mean Absolute Error (MAE)
mae2 = mean_absolute_error(test_targets, test_predictions)
mae2 = round(mae2,2)
print("MAE:", mae2, unit)

# Mean Squared Error (MSE)
mse2 = mean_squared_error(test_targets, test_predictions)
mse2 = round(mse2,2)
print("MSE:", mse2)

# Root Mean Squared Error (RMSE)
rmse2 = np.sqrt(mse2)
rmse2 = round(rmse2,2)
print("RMSE:", rmse2, unit)

# R-squared (R2)
r22 = r2_score(test_targets, test_predictions)
r22 = round(r22,3)
print("R2:", r22)

# Mean Absolute Percentage Error (MAPE)
# Scikit-learn doesn't have a built-in MAPE function
# We can calculate manually:
mape2 = np.mean(np.abs((np.array(test_targets) - np.array(test_predictions)) / np.average(test_targets))) * 100.0
mape2 = round(mape2,3)
print("NMAE:", mape2, "%")

# Parity plot
plt.figure(figsize=(10, 10))
plt.subplot(1, 2, 1)
plt.scatter(train_targets, train_predictions, alpha=0.5, label='Train')
plt.scatter(test_targets, test_predictions, alpha=0.5, label='Test')
plt.plot([min(train_targets.min(), test_targets.min()), max(train_targets.max(), test_targets.max())],
         [min(train_targets.min(), test_targets.min()), max(train_targets.max(), test_targets.max())],
         'r--')#, label='Perfect prediction')
plt.xlabel('True values')
plt.ylabel('Predicted values')
plt.title(f'MAE: {mae:2f} | MSE: {mse:2f} | RMSE: {rmse:2f} | NMAE: {mape:2f}')
plt.legend()

# Learning curve
plt.subplot(1, 2, 2)
plt.plot(range(1, num_epochs+1), train_losses, label='Train')
plt.plot(range(1, num_epochs+1), val_losses, label='Validation')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('       ')
plt.legend()

plt.suptitle(f'Train| MAE: {mae} | MSE: {mse} | RMSE: {rmse} | MAPE: {mape}% \n Test| MAE: {mae2} | MSE: {mse2} | RMSE: {rmse2} | MAPE: {mape2}% ')

plt.tight_layout()
plt.savefig(f"CNN_{mode1}_multi_e{num_epochs}_b{batchsize}_{target}.png", dpi=300,bbox_inches='tight')